# Attention Mechanisms in Deep Learning

This repository contains Jupyter notebooks demonstrating and explaining various types of attention mechanisms used in deep learning models, especially in the context of transformers.

## ðŸ“‚ Contents

- `coding_self_attention.ipynb`  
  Implements and explains the basics of self-attention, a key component of transformer models.

- `coding_mask_self_attention.ipynb`  
  Shows how masked self-attention is used in autoregressive models like GPT to prevent future token information leakage.

- `coding_alltypes_ofattention.ipynb`  
  A combined walkthrough covering different types of attention (self, masked, additive, scaled dot-product, etc.)
